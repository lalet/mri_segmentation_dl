## Setup
1. Install CfnCluster (it's a python library, so you can use pip)
2. [Configure cfncluster](http://cfncluster.readthedocs.io/en/latest/getting_started.html#configuring-cfncluster). You can find an example config in `aws/cfncluster_config`.
3. If you need a multi-user setup, you need to copy public keys to `public_keys` directory. The name format should be `username_rsa.pub` The setup script will use these keys to create users add enable ssh access.
4. Create a custom AMI: 
  - launch a EC2 instance from an ubuntu-based AMI created by CfnCluster-team (you can find the list [here](https://github.com/awslabs/cfncluster/blob/develop/amis.txt)). 
  - ssh to the instance using ubuntu user and your private key from AWS
  - clone this repo `git clone https://github.com/mibel/mri_segmentation_dl.git`
  - launch `aws/ami_setup.sh` script (it takes 10-20 minutes)
  - create [a new AMI image](http://docs.aws.amazon.com/toolkit-for-visual-studio/latest/user-guide/tkv-create-ami-from-instance.html) using the instance with the installed software
  - copy the id of the saved AMI and add it to CfnCluster config as `custom_ami` field.
5. Now you can launch your cluster using `cfncluster create default`. As soon as CfnCluster creates the cluster, you will be able to ssh it using any key paired with a key from `public_key` directory. The first part of the public key name is used as the the user name.
6. At the master node, you can use `qsub` to submit jobs. Also you can use a function from `cluster_utils` (see below).
7. At some point you may see the following error message `Unable to run job: warning: mibel's job is not allowed to run in any queue`. Usually it means that there are no computing nodes. Wait for several minutes and the system launches nodes for you.

## Usage
1. There is a shared directory /data which is available for all users. 

2. `cluster_utils` lib provides an interface for dockerizing your cmd's. 

The lib does the following things
 - add the shared data folder and the user's home into a container;
 - add the required paths to python libs (PYTHONPATH) and to binaries (PATH) to a container;
 - wrap any command to be launched inside docker on cluster

3. An example of running a cmd from python
```
from cluster_utils import run_job
cmd = ['echo "hello, world!"']
run_job(cmd, cpu=8, gpu=1, ram=64)
```
4. Also it's possible to launch a job using an (executable) python script from `./scripts/run_cmd_cobrain.py`:
```
python scripts/run_cmd.py echo hello world
```
See also `run_cmd.py --help`

5. There is a script which allows to launch experiments generated by `dpipe` lib
```
run_experiment.py ~/experiment/ -pp "/home/mibel/neuro/deep_pipe/"
```
where
- it's supposed that 
   * `deep_pipe` lib is installed to `/home/mibel/neuro/deep_pipe/`;
   * a dpipe experiment was generated at `~/experiment`.
- `-pp` flag allows to add additional python libraries inside docker. 